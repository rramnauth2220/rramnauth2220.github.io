<!DOCTYPE html>
<html class="no-js" lang="en">
<head>

    <!--- basic page needs
    ================================================== -->
    <meta charset="utf-8">
    <title>Audio Feature Extraction - Hullo.</title>
    <meta name="description" content="">
    <meta name="author" content="">

    <!-- mobile specific metas
    ================================================== -->
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- CSS
    ================================================== -->
    <link rel="stylesheet" href="../../css/base.css">
    <link rel="stylesheet" href="../../css/vendor.css">
    <link rel="stylesheet" href="../../css/main.css">

    <!-- script
    ================================================== -->
    <script src="../../js/modernizr.js"></script>
	<script src="https://code.jquery.com/jquery-1.10.2.js"></script>
	
	<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
	<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
	
	<link rel="stylesheet" href="../../js/styles/vs.css">
	<script src="../../js/highlight.pack.js"></script>
	<script>hljs.initHighlightingOnLoad();</script>

    <!-- favicons
    ================================================== -->
    <link rel="apple-touch-icon" sizes="180x180" href="../../apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="../../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../../favicon-16x16.png">

</head>

<body class="ss-bg-white">

    <!-- preloader
    ================================================== -->
    <div id="preloader">
        <div id="loader" class="dots-fade">
            <div></div>
            <div></div>
            <div></div>
        </div>
    </div>

    <div id="top" class="s-wrap site-wrapper">

       <!-- navigation bar -->
		<div id="nav-placeholder"></div>

		<script>
		$(function(){
		  $("#nav-placeholder").load("nav-code.html");
		});
		</script>
		<!-- end navigation bar-->
        <!-- site content
        ================================================== -->
        <div class="s-content content">
            <main class="row content__page">
                
                <article class="column large-full entry format-standard">

                    <div class="media-wrap entry__media">
                        <div class="entry__post-thumb">
                            <img src="images/thumbs/single/standard/standard-1000.jpg" 
                                 srcset="images/thumbs/single/standard/standard-2000.jpg 2000w, 
                                         images/thumbs/single/standard/standard-1000.jpg 1000w, 
                                         images/thumbs/single/standard/standard-500.jpg 500w" sizes="(max-width: 2000px) 100vw, 2000px" alt="">
                        </div>
                    </div>

                    <div class="content__page-header entry__header">
                        <h1 class="display-1 entry__title">
                        Algorithmic Audio Feature Extraction <em>in English</em>
                        </h1>
                        <ul class="entry__header-meta">
                            <li class="author">By <a href="https://ramnauth2220.github.io">Rebecca Ramnauth</a></li>
                            <li class="date">May 25, 2020</li>
                            <li class="cat-links">
                                <a href="category.html">Code</a><a href="../../404.html">Research</a>
                            </li>
                        </ul>
                    </div> <!-- end entry__header -->

                    <div class="entry__content">

                        <p class="lead drop-cap">
                        In order to make machines intelligent like humans, we often rely on machine learning and artificial intelligence techniques. There are several reasons why we want computers to be able to make human-like decisions. Developing smarter artificial agents and studying our interaction with them serve as an intermediary for better understanding human behavior and cognition. 
                        </p>

                        <p>
                        Despite extensive research on vision, emotion, and cognition, the human auditory system remains understudied. If our machines could hear as well as humans do, we would expect them to easily distinguish speech from music or ambient noises, to know what direction sounds are coming from, to accurately enumerate the number of sound-producing objects in the room, to differentiate between sounds that are typical and those that are noteworthy, and to effortlessly characterize important features of any given audio source. Such machines should also be able to organize what they hear, learn names for recognizable objects and styles, and retrieve memory of a sound by reference to a handful of descriptors. These machines should be able to listen and react in real time, to take appropriate action to respond to having heard a noteworthy event. 
                        </p>
						
						<p>
						These abilities present unique challenges to computer scientists (to build better machines) and psychologists (to better understand these human functions), and are collectively motivations for the emerging field of <strong>machine hearing</strong>. Regardless of any one aim, a machine requires a set of robust and discriminatory features to accurately and quickly achieve its goal. For an ML system (and arguably also for a human child), intelligence is defined by the amount and quality of training given to it. It is typical for one to reduce the size representation of the signals used to train the machine as opposed to feeding it a complete dataset. Compact representations of a signal, also called <strong>features</strong>, reduce the size of the orginal signal signficantly while still describing the signal completely and accurately. As a result, the reduced representation improves the computational and time complexities of the ML algorithm, making it more suitable for real-time applications.  
						</p>

                        <p>
                        <img src="images/inline/single-gallery-01-1000.jpg" alt="">
                        </p>

                        <h2>Audio feature extraction</h2>
    
                        <p>
                        <strong>Feature extraction</strong> is the process of highlighting the most discriminating and impactful features of a signal. The evolution of features used in audio signal processing algorithms begins with features extracted in the <em>time domain</em> (&lt; 1950s), which continue to play an important role in audio analysis and classification. Analyzing the spectrum for several features such as pitch, formants, etc. originates from the <em>frequency domain</em> (&lt; 1960s). Eventually, <em>joint time-frequency</em> techniques (&gt; 1970s) allowed for analyzing signals in both the time and frequency domains simultaneously. Since the development of <a href="https://en.wikipedia.org/wiki/Deep_learning">deep learning</a> (&gt; 2000s), <em>deep features</em> are extensively used in a diverse array of applications such as acoustic scene classification [<a href="#barchiesi-2015">1</a>] and speaker recognition [<a href="#rahmani-2018">2</a>].
						<figure>
						<img src="images/inline/audio-signal-3d.svg" alt="an audio signal explained in 3D"> <figcaption>An audio signal, explained in 3 dimensions.</figcaption>
						</figure>
                        </p>
						
						<p>
						The remainder of this article will a walkthrough on extracting some of these features using the <a href="https://essentia.upf.edu/">Essentia</a> and <a href="https://librosa.github.io/librosa/">LibROSA</a> Python libraries. It will also be an attempt of an applied computer science student to explain what she's learned about signal processing theory in the past 48 hours. Enjoy.
						</p>
						
						<h3>Time domain features</h3>
						<h4>Windowing in time domain</h4>
						<p>Before we discuss time domain features, it is necessary to introduce the concept of windowing in time domain. The simplest way to analyze a signal is in its raw form. For <em>time series signals</em> (signals that evolve with time), <strong>windowing</strong> allows us to view a short time segment of a longer signal and analyze its frequency content. Mathematically, windowing can be seen as multiplying a signal with a window function that is zero everywhere on the given signal except on the region of interest. The simplest type of window is a rectangular window such as that defined by 
						$$W(n) = \left\{ \begin{array}{ll} 1, & -\frac{M-1}{2} \leq n \leq \frac{M-1}{2} \\ 0, & otherwise \end{array} \right.$$
						</p> 
						
						<p>As the window slides over the signal in time, the size is made adaptive by changing according to characteristics of the audio signal. One problem with this is the abrupt changes or <strong>jump discontinuities</strong> in its shape at the edges of the window. The distortion is the result of the <a href="https://en.wikipedia.org/wiki/Gibbs_phenomenon">Gibbs phenomenon</a>. To solve this, we can use a window function with smooth curves like the <a href="https://en.wikipedia.org/wiki/Hann_function">Hann function</a>: 
						$$\left.\begin{array}{rl}
						W(n)=w_{0}\left(\frac{L}{N}(n-N / 2)\right) & =\frac{1}{2}\left[1-\cos \left(\frac{2 \pi n}{N}\right)\right] \\
						& =\sin ^{2}\left(\frac{\pi n}{N}\right)
						\end{array}\right\}, \quad 0 \leq n \leq N,$$ where the length of the window is \(N + 1\). In summary, these functions round the corners of rectangular windows to downgrade the edge of the signal, reducing the edge effect and therefore the Gibbs phenomenon.
						</p>
						
						<h4>Zero-crossing rate (ZCR)</h4>
						<p>The ZCR of an audio grame is defined as the rate at which the signal changes sign. ZCR is an efficient and simple way to detecting whether a speech frame is voice, unvoiced, or silent. </p>
						https://pdf.sciencedirectassets.com/271440/1-s2.0-S0003682X19X00110/1-s2.0-S0003682X19308795/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEDgaCXVzLWVhc3QtMSJIMEYCIQDe7x2ifg7NhzIQIy7SOpGuGWzmAAbJh%2FcQZlSrU2B%2BqgIhANVII%2FIZnYz0VT1DAfKNVBO0i%2FOmIRzAl5cDoe1syDbzKr0DCJH%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEQAxoMMDU5MDAzNTQ2ODY1Igy49CzK8LgGeoCvDBMqkQMe7b%2FDuW493PAkPl8y9Fco2dPP2Mt6HaX3PIHWrwKk9SKSjspWW1cFdyT%2B6HActR5SwhodOLtPauWd0I%2BF6nhiiIXtHG4eYwAgMiuCCZcla52P9NO3cqvKK8p7xUNWbfWWv6QYCzGDUivV5Y4gRJaYlOvJuKD%2BUGVQstBuo7q2KyTNYCtxImv6%2F%2BmXWLrQw1U6A%2B6V5qjHMOEJStt%2F4l7XKDHazWi5BWnf9lYP5ouX7LdE16ygRUhncUvbi04JJCRkLp%2Blv9%2BRWqCf8qSq6OwUc2%2BoxTcSee6m%2FTC9zAku5U6RXbf6MsWzGgyb1KbIag8g3cpwIUuC7xxdg%2FMkcTj5el9SIp82rJi2BsmCryzz6bel%2BTM4iLIA7XQ1ktzYc9RDLUrDr7kUlS%2Fp6ZB1vujBfOXhcLGhrjklFasC9HFPTuB%2FhFOH5QOBeVoThbxPx%2Bnoj9oR9YTGh7kMX%2B0gObnY6NsjuOYJWhXJyRC48Tc1gc8CfHyBBXH15Rqxp9roFKv3D4l1v%2FAjBcV29VkXO2Ma2jCj1K%2F2BTrqATJ%2Bi3gjZMGxz%2Faxdp357DuTO9HwrGNzfMoZbVgyqDaEwNZvAsktj4%2FGzmxvqF0WKohW%2BxigcIuUgeGLTdN9R1F8RAV%2BG1kN65%2Bk1qvJBp5QCWwp5502qQxiDC%2BgnOcdSe2tS3svuVqDyZkkDsPk0BsAUgf%2BydvCFupQReHLsLux96n9nBAKIdlKigm90iQJzJ4BHf30U0zQ7n7xfumq7yaMpGC248ER%2BAyKSuYunI2q2AUQLaVsUF0cZ7BjeGzhKovF70dInNkS264xIs%2F6CO6oxBHTLitBl0mbvTlbuSfEKuMjjZmVgFUouA%3D%3D&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20200525T171516Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTY4S6SJD72%2F20200525%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=ae7edaab6032a5e961ce60db87a13fbe8eccc920767df9cf627c034091bb9560&hash=d0480e33b012b1c2240132ba2c4eb2767bea316c3afcbdaf5091b39a5edbbbdd&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S0003682X19308795&tid=spdf-6a53923c-da38-48a5-bec4-fb6ce51cd002&sid=8cd4c85a3bd2f24f49-955e-afd6f0b4c9d9gxrqa&type=client
						
						<pre><code class="python">import essentia
from essentia.standard import *
import essentia.standard as es
print("Hullo wurld")
</code></pre>
						
						<h2>References</h2>
						<ol>
							<li id="#barchiesi-2015">Barchiesi, D., Giannoulis, D., Stowell, D., & Plumbley, M. D. (2015). Acoustic scene classification: Classifying environments from the sounds they produce. <em>IEEE Signal Processing Magazine, 32</em>(3), 16-34. [<a href="https://ieeexplore.ieee.org/abstract/document/7760424/?casa_token=feP8fRrzDdwAAAAA:njTGJ3xScXbqpSiW_SO_6ry9_-cmcEsnNgW9fUc3ZAi_Jb-llci5oEKhLeOyNNC2boM-bo2pxCU">PDF</a>]</li>
							<li id="#rahmani-2018">Rahmani, M. H., Almasganj, F., & Seyyedsalehi, S. A. (2018). Audio-visual feature fusion via deep neural networks for automatic speech recognition. <em>Digital Signal Processing, 82,</em> 54-63. [<a href="https://www.sciencedirect.com/science/article/pii/S1051200418305050?casa_token=NHMQ69s4NYUAAAAA:ioRv7aYl7BiFOT57galxXoyUW1AleAJBkqFADl_1mv7tgUnMcQpzSXElVZ8PHRq_vB7qOLBosP4">PDF</a>]</li>
						</ol>
    

                        <p class="entry__tags">
                            <span>Post Tags</span>
        
                            <span class="entry__tag-list">
                                <a href="#0">python</a>
                                <a href="#0">essentia</a>
                                <a href="#0">librosa</a>
                                <a href="#0">audio</a>
                            </span>
            
                        </p>
                    </div> <!-- end entry content -->

					<!--
                    <div class="entry__pagenav">
                        <div class="entry__nav">
                            <div class="entry__prev">
                                <a href="#0" rel="prev">
                                    <span>Previous Post</span>
                                    Tips on Minimalist Design 
                                </a>
                            </div>
                            <div class="entry__next">
                                <a href="#0" rel="next">
                                    <span>Next Post</span>
                                    Less Is More 
                                </a>
                            </div>
                        </div>
                    </div>

                    <div class="entry__related">
                        <h3 class="h2">Related Articles</h3>

                        <ul class="related">
                            <li class="related__item">
                                <a href="single-standard.html" class="related__link">
                                    <img src="images/thumbs/masonry/walk-600.jpg" alt="">
                                </a>
                                <h5 class="related__post-title">Using Repetition and Patterns in Photography.</h5>
                            </li>
                            <li class="related__item">
                                <a href="single-standard.html" class="related__link">
                                    <img src="images/thumbs/masonry/dew-600.jpg" alt="">
                                </a>
                                <h5 class="related__post-title">Health Benefits Of Morning Dew.</h5>
                            </li>
                            <li class="related__item">
                                <a href="single-standard.html" class="related__link">
                                    <img src="images/thumbs/masonry/rucksack-600.jpg" alt="">
                                </a>
                                <h5 class="related__post-title">The Art Of Visual Storytelling.</h5>
                            </li>
                        </ul>
                    </div>

                </article> -->
            </main>

        </div> <!-- end s-content -->

    </div> <!-- end s-wrap -->


    <!-- Java Script
    ================================================== -->
    <script src="../../js/jquery-3.2.1.min.js"></script>
    <script src="../../js/plugins.js"></script>
    <script src="../../js/main.js"></script>

</body>