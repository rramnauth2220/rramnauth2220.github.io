<!DOCTYPE html>
<html class="no-js" lang="en">
<head>

    <!--- basic page needs
    ================================================== -->
    <meta charset="utf-8">
    <title>Audio Feature Extraction - Hullo.</title>
    <meta name="description" content="">
    <meta name="author" content="">

    <!-- mobile specific metas
    ================================================== -->
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- CSS
    ================================================== -->
    <link rel="stylesheet" href="../../css/base.css">
    <link rel="stylesheet" href="../../css/vendor.css">
    <link rel="stylesheet" href="../../css/main.css">

    <!-- script
    ================================================== -->
    <script src="../../js/modernizr.js"></script>
	<script src="https://code.jquery.com/jquery-1.10.2.js"></script>
	
	<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
	<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
	
	<link rel="stylesheet" href="../../js/styles/vs.css">
	<script src="../../js/highlight.pack.js"></script>
	<script>hljs.initHighlightingOnLoad();</script>

    <!-- favicons
    ================================================== -->
    <link rel="apple-touch-icon" sizes="180x180" href="../../apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="../../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../../favicon-16x16.png">

</head>

<body class="ss-bg-white">

    <!-- preloader
    ================================================== -->
    <div id="preloader">
        <div id="loader" class="dots-fade">
            <div></div>
            <div></div>
            <div></div>
        </div>
    </div>

    <div id="top" class="s-wrap site-wrapper">

       <!-- navigation bar -->
		<div id="nav-placeholder"></div>

		<script>
		$(function(){
		  $("#nav-placeholder").load("nav-code.html");
		});
		</script>
		<!-- end navigation bar-->
        <!-- site content
        ================================================== -->
        <div class="s-content content">
            <main class="row content__page">
                
                <article class="column large-full entry format-standard">

                    <div class="media-wrap entry__media">
                        <div class="entry__post-thumb">
                            <img src="images/thumbs/single/standard/standard-1000.jpg" 
                                 srcset="images/thumbs/single/standard/standard-2000.jpg 2000w, 
                                         images/thumbs/single/standard/standard-1000.jpg 1000w, 
                                         images/thumbs/single/standard/standard-500.jpg 500w" sizes="(max-width: 2000px) 100vw, 2000px" alt="">
                        </div>
                    </div>

                    <div class="content__page-header entry__header">
                        <h1 class="display-1 entry__title">
                        Algorithmic Audio Feature Extraction <em>in English</em>
                        </h1>
                        <ul class="entry__header-meta">
                            <li class="author">By <a href="https://ramnauth2220.github.io">Rebecca Ramnauth</a></li>
                            <li class="date">May 25, 2020</li>
                            <li class="cat-links">
                                <a href="category.html">Code</a><a href="../../404.html">Research</a>
                            </li>
                        </ul>
                    </div> <!-- end entry__header -->

                    <div class="entry__content">

                        <p class="lead drop-cap">
                        In order to make machines intelligent like humans, we often rely on machine learning and artificial intelligence techniques. There are several reasons why we want computers to be able to make human-like decisions. Developing smarter artificial agents and studying our interaction with them serve as an intermediary for better understanding human behavior and cognition. 
                        </p>

                        <p>
                        Despite extensive research on vision, emotion, and cognition, the human auditory system remains understudied. If our machines could hear as well as humans do, we would expect them to easily distinguish speech from music or ambient noises, to know what direction sounds are coming from, to accurately enumerate the number of sound-producing objects in the room, to differentiate between sounds that are typical and those that are noteworthy, and to effortlessly characterize important features of any given audio source. Such machines should also be able to organize what they hear, learn names for recognizable objects and styles, and retrieve memory of a sound by reference to a handful of descriptors. These machines should be able to listen and react in real time, to take appropriate action to respond to having heard a noteworthy event. 
                        </p>
						
						<p>
						These abilities present unique challenges to computer scientists (to build better machines) and psychologists (to better understand these human functions), and are collectively motivations for the emerging field of <strong>machine hearing</strong>. Regardless of any one aim, a machine requires a set of robust and discriminatory features to accurately and quickly achieve its goal. For an ML system (and arguably also for a human child), intelligence is defined by the amount and quality of training given to it. It is typical for one to reduce the size representation of the signals used to train the machine as opposed to feeding it a complete dataset. Compact representations of a signal, also called <strong>features</strong>, reduce the size of the orginal signal signficantly while still describing the signal completely and accurately. As a result, the reduced representation improves the computational and time complexities of the ML algorithm, making it more suitable for real-time applications.  
						</p>

                        <p>
                        <img src="images/inline/single-gallery-01-1000.jpg" alt="">
                        </p>

                        <h2>Audio feature extraction</h2>
    
                        <p>
                        <strong>Feature extraction</strong> is the process of highlighting the most discriminating and impactful features of a signal. The evolution of features used in audio signal processing algorithms begins with features extracted in the <em>time domain</em> (&lt; 1950s), which continue to play an important role in audio analysis and classification. Analyzing the spectrum for several features such as pitch, formants, etc. originates from the <em>frequency domain</em> (&lt; 1960s). Eventually, <em>joint time-frequency</em> techniques (&gt; 1970s) allowed for analyzing signals in both the time and frequency domains simultaneously. Since the development of <a href="https://en.wikipedia.org/wiki/Deep_learning">deep learning</a> (&gt; 2000s), <em>deep features</em> are extensively used in a diverse array of applications such as acoustic scene classification [<a href="#barchiesi-2015">1</a>] and speaker recognition [<a href="#rahmani-2018">2</a>].
						<figure>
						<img src="images/inline/audio-signal-3d.svg" alt="an audio signal explained in 3D"> <figcaption>An audio signal, explained in 3 dimensions.</figcaption>
						</figure>
                        </p>
						
						<p>
						The remainder of this article will a walkthrough on extracting some of these features using the <a href="https://essentia.upf.edu/">Essentia</a> and <a href="https://librosa.github.io/librosa/">LibROSA</a> Python libraries. It will also be an attempt of an applied computer science student to explain what she's learned about signal processing theory in the past 48 hours. Enjoy.
						</p>
						
						<h3>Time domain features</h3>
                        <pre><code class="python">%matplotlib inline  
import librosa                     # librosa music package
import essentia                    # essentia music package
import essentia.standard           # essentia for imperative programming
import essentia.streaming          # essentia for declarative programming

import IPython                     # for playing audio
import numpy as np                 # for handling large arrays
import pandas as pd                # for data manipulation and analysis
import scipy                       # for common math functions
import sklearn                     # a machine learning library
import os                          # for accessing local files

import librosa.display             # librosa plot functions
import matplotlib.pyplot as plt    # plotting with Matlab functionality
import seaborn as sns              # data visualization based on matplotlib
</code></pre>

<pre><code class="python"># in libROSA
audio_librosa = 'audio/human-rights.wav' # audio file path
y, sr = librosa.core.load(audio_librosa) # load samples and determine sampling rate

# in Essentia
loader = essentia.standard.MonoLoader(filename='audio/human-rights.wav') # instantiate audio loader
audio_essentia = loader() # actually load audio file

# to hear how the audio we want to process sounds like
IPython.display.Audio('audio/human-rights.wav')
</code></pre>

<pre><code class="python">plt.rcParams['figure.figsize'] = (15, 6) # instantiate plot canvas
plot(audio[1*44100:2*44100]) # get the 2nd second of the audio clip
show()
</code></pre>
<img src="images/inline/audio-plot-1.png" alt="time series of audio clip"/>
                        <h4>Windowing in time domain</h4>
						<p>Before we discuss time domain features, it is necessary to introduce the concept of windowing in time domain. The simplest way to analyze a signal is in its raw form. For <em>time series signals</em> (signals that evolve with time), <strong>windowing</strong> allows us to view a short time segment of a longer signal and analyze its frequency content. Mathematically, windowing can be seen as multiplying a signal with a window function that is zero everywhere on the given signal except on the region of interest. The simplest type of window is a rectangular window such as that defined by 
						$$W(n) = \left\{ \begin{array}{ll} 1, & -\frac{M-1}{2} \leq n \leq \frac{M-1}{2} \\ 0, & otherwise \end{array} \right.$$
						</p> 
						
						<p>As the window slides over the signal in time, the size is made adaptive by changing according to characteristics of the audio signal. One problem with this is the abrupt changes or <strong>jump discontinuities</strong> in its shape at the edges of the window. The distortion is the result of the <a href="https://en.wikipedia.org/wiki/Gibbs_phenomenon">Gibbs phenomenon</a>. To solve this, we can use a window function with smooth curves like the <a href="https://en.wikipedia.org/wiki/Hann_function">Hann function</a>: 
						$$\left.\begin{array}{rl}
						W(n)=w_{0}\left(\frac{L}{N}(n-N / 2)\right) & =\frac{1}{2}\left[1-\cos \left(\frac{2 \pi n}{N}\right)\right] \\
						& =\sin ^{2}\left(\frac{\pi n}{N}\right)
						\end{array}\right\}, \quad 0 \leq n \leq N,$$ where the length of the window is \(N + 1\). In summary, these functions round the corners of rectangular windows to downgrade the edge of the signal, reducing the edge effect and therefore the Gibbs phenomenon.
                        </p>
                        <pre><code class="python">w_essentia = Windowing(type = 'hann')    # specify window function in essentia
w_scipy = signal.get_window('triang', 7) # apply window function in scipy
w_librosa= signal.get_window('hamm', 7)  # apply window function in librosa 
</code></pre>
<p>The windowing method in LibROSA is provided as a wrapper for the scipy function of the same name that also supports callable or pre-computed windows. For a list of window types supported by scipy and LibROSA, see <a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.get_window.html#scipy.signal.get_window">scipy.signal.get_window</a>. For a list of types supported by Essentia, see <a href="https://essentia.upf.edu/reference/std_Windowing.html">essentia.standard.Windowing</a>.</p>
						
						<h4>Zero-crossing rate (ZCR)</h4>
                        <p>The ZCR of an audio grame is defined as the rate at which the signal changes sign. ZCR is an efficient and simple way to detecting whether a speech frame is voice, unvoiced, or silent. It is expected that unvoiced segments produce higher ZCRs than for voice segments, and ideally ZCRs equal to zero for silence segments [<a href="#bachu-2010">3</a>]. </p>

                        <p>ZCR is also an essential technique for estimating <strong>fundamental frequency</strong>, the lowest frequency of a periodic waveform. The ZCR feature can therefore be used to design discriminator and classifier systems. With this additional information, this feature demonstrates, for example, a higher ZCR for music sources than for speech source. To see the implementation of such a system, check out <a href="https://ieeexplore.ieee.org/document/1386250">"A speech/music discriminator based on RMS and zero-crossings."</a></p>
						
						<pre><code class="python"># SETUP
w = Windowing(type = 'hann')    # state windowing function
spectrum = Spectrum()           # instead of a complex FTT (the output of FFT()), we just want the magnitude spectrum
zcr = ZeroCrossingRate()        # instantiate the essentia.standard zero-crossing function

zcrs = []
frameSize = 1024
hopSize = 512
</code></pre>

<code><pre># APPLY ZCR FUNCTION to SPEECH signal

# iterating through frames matlab style:
# for fstart in range(0, len(audio)-frameSize, hopSize):
#   frame = audio[fstart:fstart+frameSize]

# iterating through frames essentia style:
for frame in FrameGenerator(audio, frameSize=1024, hopSize=512, startFromZero=True):
    zcrs.append(zcr(spectrum(w(frame))))

zcrs = essentia.array(zcrs).T # shape output array
</code></pre>

<code><pre># PLOT OUTPUT
zcrs_x = range(0, len(zcrs))
zcrs_y = zcrs
plt.plot(zcrs_x, zcrs_y, 'o', color='black') # plot ZCR per frame as points
plt.plot(zcrs) # plot time-series change in ZCRs
show()
</code></pre>
<img src="images/inline/audio-zcrs.png"/>

<pre><code class="python"># APPLY LibROSA ZCR FUNCTION to MUSIC signal
zcr = librosa.feature.zero_crossing_rate(y) 

# PLOT OUTPUT
plt.figure(figsize=(15,5))
plt.semilogy(zcr.T, label='Fraction') # apply log transform on y
plt.ylabel('Fraction per Frame')
plt.xticks([])
plt.xlim([0, rolloff.shape[-1]])
plt.legend()    
</code></pre>
<img src="images/inline/audio-zcrs-librosa.png"/>


						<h2>References</h2>
						<ol>
							<li id="#barchiesi-2015">Barchiesi, D., Giannoulis, D., Stowell, D., & Plumbley, M. D. (2015). Acoustic scene classification: Classifying environments from the sounds they produce. <em>IEEE Signal Processing Magazine, 32</em>(3), 16-34. [<a href="https://ieeexplore.ieee.org/abstract/document/7760424/?casa_token=feP8fRrzDdwAAAAA:njTGJ3xScXbqpSiW_SO_6ry9_-cmcEsnNgW9fUc3ZAi_Jb-llci5oEKhLeOyNNC2boM-bo2pxCU">PDF</a>]</li>
							<li id="#rahmani-2018">Rahmani, M. H., Almasganj, F., & Seyyedsalehi, S. A. (2018). Audio-visual feature fusion via deep neural networks for automatic speech recognition. <em>Digital Signal Processing, 82,</em> 54-63. [<a href="https://www.sciencedirect.com/science/article/pii/S1051200418305050?casa_token=NHMQ69s4NYUAAAAA:ioRv7aYl7BiFOT57galxXoyUW1AleAJBkqFADl_1mv7tgUnMcQpzSXElVZ8PHRq_vB7qOLBosP4">PDF</a>]</li>
                            <li id="#bachu-2010">Bachu, R. G., Kopparthi, S., Adapa, B., & Barkana, B. D. (2010). Voiced/unvoiced decision for speech signals based on zero-crossing rate and energy. <em>In Advanced Techniques in Computing Sciences and Software Engineering </em>(pp. 279-282). Springer, Dordrecht. [<a href="https://www.researchgate.net/profile/Buket_Barkana/publication/259823741_VoicedUnvoiced_Decision_for_Speech_Signals_Based_on_Zero-Crossing_Rate_and_Energy/links/5654b2bf08ae4988a7b06035.pdf">PDF</a>]</li>
                        </ol>
    

                        <p class="entry__tags">
                            <span>Post Tags</span>
        
                            <span class="entry__tag-list">
                                <a href="#0">python</a>
                                <a href="#0">essentia</a>
                                <a href="#0">librosa</a>
                                <a href="#0">audio</a>
                            </span>
            
                        </p>
                    </div> <!-- end entry content -->

					<!--
                    <div class="entry__pagenav">
                        <div class="entry__nav">
                            <div class="entry__prev">
                                <a href="#0" rel="prev">
                                    <span>Previous Post</span>
                                    Tips on Minimalist Design 
                                </a>
                            </div>
                            <div class="entry__next">
                                <a href="#0" rel="next">
                                    <span>Next Post</span>
                                    Less Is More 
                                </a>
                            </div>
                        </div>
                    </div>

                    <div class="entry__related">
                        <h3 class="h2">Related Articles</h3>

                        <ul class="related">
                            <li class="related__item">
                                <a href="single-standard.html" class="related__link">
                                    <img src="images/thumbs/masonry/walk-600.jpg" alt="">
                                </a>
                                <h5 class="related__post-title">Using Repetition and Patterns in Photography.</h5>
                            </li>
                            <li class="related__item">
                                <a href="single-standard.html" class="related__link">
                                    <img src="images/thumbs/masonry/dew-600.jpg" alt="">
                                </a>
                                <h5 class="related__post-title">Health Benefits Of Morning Dew.</h5>
                            </li>
                            <li class="related__item">
                                <a href="single-standard.html" class="related__link">
                                    <img src="images/thumbs/masonry/rucksack-600.jpg" alt="">
                                </a>
                                <h5 class="related__post-title">The Art Of Visual Storytelling.</h5>
                            </li>
                        </ul>
                    </div>

                </article> -->
            </main>

        </div> <!-- end s-content -->

    </div> <!-- end s-wrap -->


    <!-- Java Script
    ================================================== -->
    <script src="../../js/jquery-3.2.1.min.js"></script>
    <script src="../../js/plugins.js"></script>
    <script src="../../js/main.js"></script>

</body>